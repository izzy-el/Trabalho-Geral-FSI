# -*- coding: utf-8 -*-
"""Trab_geralzao.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XRFMalVwhBIL6atXpjAxUme5Rsb4oY9g

https://www.kaggle.com/uciml/student-alcohol-consumption
"""

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import cross_val_score

#tabela sobre alunos entrevistados sobre a condição de vida e o progresso acadêmico.
stud = pd.read_csv('student-por.csv')

stud.head()

#usando o metodo loc para selecionar colunas específicas da tabela para analisar
stud.loc[0:,['school','sex','age','Pstatus','guardian','studytime','failures','famsup','higher','famrel','goout','Dalc','Walc', 'G3']]

#usando o metodo describe na "nova tabela"
#o modelo é sobre a tendência do aluno a ter uma nota pior
stud.loc[0:,['school','sex','age','Pstatus','guardian','studytime','failures','famsup','higher','famrel','goout','Dalc','Walc', 'G3']].describe()

#O numero de garotos e garotas entrevistados é quase o mesmo (aproximadamente 200 cada)
sns.countplot(x='sex', data=stud)

#A maior parte dos alunos entrevistados são da escola GP 
sns.countplot(x='school', data=stud)

#relacionando a coluna "sexo" com a intenção de cursar "ensino superior"
#aqueles que não pretendem fazer ensino superior são mejoritariamente homens.
sns.factorplot(x="sex", col="higher", kind="count", data=stud)

#mostrando a comparação entre a nota e final e a inteção de cursar o ensino superior.
#podemos perceber que aqueles que não possuem a intenção de fazer um curso superior tendem a ter notas mais baixas do que aqueles que planejam.
sns.catplot(x="G3", col="higher", kind="count", data=stud)

#mostrando a quantidade de pessoas que consomen pouco/muito álcool
sns.countplot(x='Walc', data=stud)

#comparando a nota final com o consumo de álcool.
#percebemos que os alunos que consomem mais álcool tendem a ter uma nota mais baixa.
#a frequência de notas maiores ocorre no walc1 até o walc3.
#a frequência de notas menores ocorre no walc4 e walc5.
sns.catplot(x="G3", col="Walc", kind="count", data=stud)

#A tendencia é de homens beberem mais do que mulheres
sns.catplot(x="Walc", col="sex", kind="count", data=stud)

#A maior parte dos alunos marcou que tem um bom relacionamento com a familia (4)
sns.countplot(x='famrel',data=stud)

#A tendencia é de que quanto melhor o relacionamento com a familia, menor a frequencia que o aluno bebe em finais de semana
sns.catplot(x="Walc", col="famrel", kind="count", data=stud)

#A quantidade de alunos entrevistados com pais separados é muito pequena
sns.countplot(x='Pstatus',data=stud)

#A tendencia é de que alunos com pais separados bebam mais em finais de semana
sns.catplot(x="Walc", col="Pstatus", kind="count", data=stud)

#A maior parte dos alunos entrevistados marcou o 3 como frequencia de saídas
sns.countplot(x='goout',data=stud)

#Alunos com 16-18 anos tendem a sair mais em relação aos outros
sns.catplot(x="age", col="goout", kind="count", data=stud)

#Alunos com 17-18 anos tendem a beber mais
sns.catplot(x="age", col="Walc", kind="count", data=stud)

#Alunos que saem mais tem tendencia a beber mais
sns.catplot(x="Walc", col="goout", kind="count", data=stud)

#Alunos que estudam na escola MS tem tendencia a beber mais
sns.catplot(x="Walc", col="school", kind="count", data=stud)

"""Homens

Não quer ensino superior

Relacionamento Familiar Ruim

Pais Separados

17-18 anos

Sai mais

Estuda na escola MS

= Tendencia a ter notas piores

________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

### Passo 1 - Escolha uma classe de modelo importando do Scikit.
"""

from sklearn.ensemble import RandomForestClassifier

"""### Passo 2 - Escolha os hiperparâmetros do modelo instanciando a classe escolhida em 1."""

model_forest = RandomForestClassifier()

"""### Passo 3 - Organize os dados em uma feature matrix e um target array."""

# Passo 3.1 - Feature Matrix
X_stud = stud.loc[ : , ["famrel", "Dalc", "G3"]]
X_stud.head()

# Passo 3.2 - Target Array
y_stud = stud.higher
y_stud.head()

# Passo 3.3 - Separação do banco de dados
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_stud, y_stud, random_state=42)

"""### Passo 4 - Ajuste o modelo invocando o método fit()."""

model_forest.fit(X_train, y_train) #Passo 4

"""### Passo 5 - Aplique o modelo aos novos dados usando predict()."""

y_model_forest = model_forest.predict(X_test)

#Veja como ficou o modelo após aplicar a classificação
y_model_forest

#Veja como teria que ser o modelo teóricamente (se estivesse correto)
y_test

"""### Testando a acuracia"""

accuracy_score(y_test, y_model_forest)

cross_val_score(model_forest, X_stud, y_stud, cv=5)

#Tecnica leave one out
from sklearn.model_selection import LeaveOneOut
cross_val_score(model_forest, X_stud, y_stud, cv=LeaveOneOut()).mean()

"""### Matriz de confusão"""

#Verificando onde estão os erros
#nesse caso ele colocou 11 pessoas que não querem fazer ensino superior como se quisessem
#e 2 que gostariam de fazer, como se não quisessem
from sklearn.metrics import confusion_matrix

mat_forest = confusion_matrix(y_test, y_model_forest)
sns.heatmap(mat_forest, square=True, annot=True)
plt.xlabel('Valores Preditos')
plt.ylabel('Valores Reais')

"""# Outros modelos

## KNeighbors Classifier
"""

#Passo 1
from sklearn.neighbors import KNeighborsClassifier 
#Passo 2
model_neighbors = KNeighborsClassifier(n_neighbors=1)
#Passo 3 (ja foi feito)
#Passo 4
model_neighbors.fit(X_train, y_train)
#Passo 5
y_model_neighbors = model_neighbors.predict(X_test)

#Testando acurácia
cross_val_score(model_neighbors, X_stud, y_stud, cv=5)

mat_neigh = confusion_matrix(y_test, y_model_neighbors)
sns.heatmap(mat_neigh, square=True, annot=True)
plt.xlabel('Valores Preditos')
plt.ylabel('Valores Reais')

"""## Decision Tree"""

from sklearn.tree import DecisionTreeClassifier # Passo 1

model_tree = DecisionTreeClassifier() # Passo 2

#Passo 3 ja foi feito

model_tree.fit(X_train, y_train) #Passo 4

y_model_tree = model_tree.predict(X_test) #Passo 5

#Testar acurácia
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_model_tree)

cross_val_score(model_tree, X_stud, y_stud, cv=5)

#Matriz de confusão
from sklearn.metrics import confusion_matrix

mat_tree = confusion_matrix(y_test, y_model_tree)
sns.heatmap(mat_tree, square=True, annot=True)
plt.xlabel('Valores Preditos')
plt.ylabel('Valores Reais')

"""## Bayes"""

from sklearn.naive_bayes import GaussianNB #Passo 1
model_bayes = GaussianNB()#Passo 2
#Passo 3 ja foi feito
model_bayes.fit(X_train, y_train)#Passo 4
y_model_bayes = model_bayes.predict(X_test)#Passo 5

#Testando acurácia
accuracy_score(y_test, y_model_bayes)

cross_val_score(model_bayes, X_stud, y_stud, cv=5)

#Tecnica leave one out
from sklearn.model_selection import LeaveOneOut
cross_val_score(model_bayes, X_stud, y_stud, cv=LeaveOneOut()).mean()

#Matriz de confusão
from sklearn.metrics import confusion_matrix

mat_bayes = confusion_matrix(y_test, y_model_bayes)
sns.heatmap(mat_bayes, square=True, annot=True)
plt.xlabel('Valores Preditos')
plt.ylabel('Valores Reais')
